
Input:
# Task
You carefully read the contents of the “Paper Outline” and select one GitHub link from the “GitHub URLs List” that you think is most relevant to the contents.
# Constraints
- Output the index number corresponding to the selected GitHub URL.
- Be sure to select only one GitHub URL.
- If there is no related GitHub link, output None.
# Paper Outline
The core methodology is an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose new preference optimization loss functions as PyTorch code. The process starts by providing the LLM with established loss functions and their performance as in-context examples. Each proposed function undergoes unit tests for validity. Valid functions are used to fine-tune an LLM, and their performance is evaluated on a predefined downstream validation task (MT-Bench). This performance metric is then fed back to the LLM to refine subsequent proposals. This meta-optimization approach aims to discover novel learning algorithms by leveraging the LLM's knowledge to generate code-level objective functions directly. The discovered DiscoPOP (LRML) dynamically weights logistic and exponential losses based on a sigmoid calculation of the difference of log-ratios, with a temperature parameter.

# GitHub URLs List
['https://github.com/luchris429/DiscoPOP', 'https://github.com/tatsu-lab/alpaca_eval', 'https://github.com/tatsu-lab/alpaca_eval', 'https://github.com/vanderschaarlab/DiscoPOP', 'https://github.com/lm-sys/FastChat', 'https://github.com/tatsu-lab/alpaca_eval']
Output:
{
    "index": 0
}
