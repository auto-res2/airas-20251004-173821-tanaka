
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "Alternative strictly-proper scoring rules (e.g. Brier, Spherical) improve calibration and downstream generation when used in fine-tuning, but their bounded nature gives weak gradients, slowing convergence and sometimes hurting perplexity. Conversely, the usual cross-entropy (logarithmic score) learns fast but produces over-confident, poorly calibrated probabilities. The key open problem is to obtain fast learning and good perplexity of cross-entropy while retaining the calibration benefits of bounded scores – ideally through a very small change to the loss.",
    "Methods": "We propose Hybrid Proper Loss (HyPro): a convex combination of the token-level Cross-Entropy (CE) and the Brier score.  \nToken at time t with true index y and predicted distribution p:\n  CE_t = − log p_y\n  Brier_t = (1 − p_y)^2 + Σ_{j≠y} p_j^2\n  HyPro_t = (1 − α) · CE_t + α · Brier_t ,  0 ≤ α ≤ 1\n\nThe mixture of strictly proper scores is itself strictly proper, so HyPro remains a valid training objective.  \nTheoretical motivation:\n1. Early training requires strong gradients ⇒ keep CE dominant.\n2. Later training benefits from calibration ⇒ slowly increase α.\nWe therefore schedule α linearly from 0 to α_max (e.g. 0.3) over the first K% of steps.\nNo other change to optimiser, model, or data is required.",
    "Experimental Setup": "Model: GPT-2 small (117 M).  \nDataset: WikiText-2 for language modeling fine-tuning (train/valid/test splits).  \nBaselines: (a) Cross-Entropy, (b) pure Brier (α=1).  \nProposed: HyPro with α scheduled 0→0.3 over first 50% steps.\nMetrics:  \n1. Perplexity (PPL) on validation/test.  \n2. Expected Calibration Error (ECE) with 10 bins.  \nTraining: 3 epochs, LR 5e-5, AdamW, batch 8, context length 512.  \nCompare PPL and ECE of HyPro against baselines.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef brier_loss(logits, targets):\n    # logits: [B, V], targets: [B]\n    probs = torch.softmax(logits, dim=-1)\n    one_hot = F.one_hot(targets, logits.size(-1)).float()\n    return ((probs - one_hot) ** 2).sum(dim=-1)  # [B]\n\ndef hybrid_loss(logits, targets, alpha):\n    ce = F.cross_entropy(logits, targets, reduction='none')\n    br = brier_loss(logits, targets)\n    return (1 - alpha) * ce + alpha * br  # [B]\n\n# training step skeleton\na_max = 0.3\nwarmup_fraction = 0.5\n\ndef get_alpha(step, total_steps):\n    progress = step / (total_steps * warmup_fraction)\n    return min(a_max, a_max * progress)\n\n# in the training loop\nfor step, (input_ids, targets) in enumerate(loader):\n    logits = model(input_ids).logits[:, -1]  # last token prediction\n    alpha = get_alpha(step, total_steps)\n    loss = hybrid_loss(logits, targets, alpha).mean()\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()",
    "Expected Result": "1. Perplexity: HyPro ≈ CE (within ±0.5 PPL) and markedly better than pure Brier.\n2. Calibration: HyPro reduces ECE by 20-30 % relative to CE and is close to (or slightly better than) pure Brier.\nThus, HyPro achieves CE-like likelihood performance and Brier-like calibration simultaneously.",
    "Expected Conclusion": "A simple weighted mix of two existing strictly proper losses yields the best of both worlds: fast, stable learning and well-calibrated output probabilities. Because the modification is a single extra line in the loss computation, it can be dropped into any current LLM training or fine-tuning pipeline, providing immediate practical benefits without hyper-parameter retuning or architectural changes."
}

# Related Works
{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This paper introduces an LLM-driven objective discovery pipeline to automatically generate novel state-of-the-art offline preference optimization algorithms for Large Language Models (LLMs) without expert human intervention. The main research problem addressed is the under-exploration of the vast search space of possible loss functions in preference optimization, traditionally constrained by human creativity. The key finding is the discovery of Discovered Preference Optimization (DiscoPOP), a novel algorithm (also referred to as Log Ratio Modulated Loss, LRML) that adaptively blends logistic and exponential losses. Experiments demonstrate DiscoPOP's state-of-the-art performance and its successful transfer to held-out tasks.",
    "Methodology": "The core methodology is an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose new preference optimization loss functions as PyTorch code. The process starts by providing the LLM with established loss functions and their performance as in-context examples. Each proposed function undergoes unit tests for validity. Valid functions are used to fine-tune an LLM, and their performance is evaluated on a predefined downstream validation task (MT-Bench). This performance metric is then fed back to the LLM to refine subsequent proposals. This meta-optimization approach aims to discover novel learning algorithms by leveraging the LLM's knowledge to generate code-level objective functions directly. The discovered DiscoPOP (LRML) dynamically weights logistic and exponential losses based on a sigmoid calculation of the difference of log-ratios, with a temperature parameter.",
    "Experimental Setup": "The LLM-driven discovery process was performed using GPT-4. The base LLM for preference optimization was `zephyr-7b-gemma-sft` (a 7 billion Gemma model fine-tuned on `deita-10k-v0-sft`), and for the IMDb task, a GPT-2 model fine-tuned on the IMDb dataset. Training utilized a fixed `β = 0.05` during discovery, a learning rate of 5e-7, bfloat16, two epochs, a batch size of two per device, gradient accumulation of eight, a cosine learning rate scheduler, and the AdamW optimizer, implemented with the TRL transformers library. Models were trained on Nvidia A100 GPUs (8 for discovery, 4 for summarization and IMDb). The discovery task focused on multi-turn dialogue, evaluated on MT-Bench using GPT-4 as a judge. Held-out tasks included single-turn dialogue (Alpaca Eval 2.0, evaluated against GPT-4 or SFT base model with win rates), summarization (TL;DR dataset, custom evaluation with Alpaca Eval 2.0 library and GPT-4 annotator), and positive sentiment generation (IMDb, evaluated using a pre-trained sentiment classifier and KL-Divergence across various `β` values).",
    "Limitations": "The current approach has several limitations: the exploration of LLM objective proposal generation techniques is still nascent; DiscoPOP struggles to converge when the `β` parameter is too low (≤ 0.01) or too high (≥ 2.5), likely because the discovery process was fixed at `β=0.05`; the highest-performing loss re-purposed `β`, affecting both functional behavior and KL penalty; the use of closed-source models (GPT-4) for code generation limits reproducibility and is costly; LLM outputs are prone to hallucinations, necessitating content filters; and the work takes a step towards code-level self-improvement, which could lead to unintended behaviors.",
    "Future Research Directions": "Future work could focus on more effective LLM objective proposal generation techniques, such as leveraging entire learning curve plots with Visual Language Models or meta-meta-optimizing the LLM prompt. Exploring objective functions with multiple floating-point parameters that can be tuned separately, beyond a single `β` parameter, is also suggested. Reformulating the objective to enable multi-parameter analysis would be beneficial. Furthermore, using the produced models themselves to generate code for self-improvement is a potential avenue. Lastly, constraining the exploring LLM to ensure the `β` multiplication with the difference of log-ratios `ρ` before other calculations is crucial.",
    "Experiment Code": "    def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The iterative LLM-driven objective discovery pipeline uses GPT-4 as the LLM for proposing new preference optimization loss functions. The process is initiated with an archive of established loss functions (DPO, HINGE, IPO, KTO) and their MT-Bench performance scores as in-context examples.\n\nEach proposed loss function undergoes a unit test for validity, checking for proper function definition, callable status, output shape of (10,), and absence of NaNs in both the loss output and its gradients after a backward pass.\n\nValid loss functions are then used to fine-tune a model (e.g., Zephyr 2B or 7B Gemma) using the `scripts/run_gpo.py` script, leveraging `accelerate launch` with `deepspeed_zero3.yaml` for distributed training. Training parameters include `gradient_accumulation_steps=16` (when 4 GPUs are used) and a `beta` parameter fixed at `0.05` for the loss functions.\n\nThe fine-tuned models' performance is evaluated on the MT-Bench downstream validation task using FastChat's `llm_judge` tools (`gen_model_answer.py` and `gen_judgment.py`). The average score from MT-Bench serves as the fitness metric, which is then fed back to GPT-4 to inform and refine subsequent loss function proposals.\n\nThe discovered DiscoPOP (LRML) loss function itself incorporates a temperature parameter `tau = 0.05` to modulate the mixing coefficient between its logistic and exponential components based on a sigmoid calculation of the difference of log-ratios."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper proposes a novel strategy to adapt any non-local strictly proper scoring rules for language generation, addressing the limitation of the widely used logarithmic score (cross-entropy loss) in handling exponentially large sample spaces. It demonstrates that training language generation models, including large language models (LLMs) like LLaMA-7B and LLaMA-13B, with alternative strictly proper scoring rules (Brier score and Spherical score) can yield substantial improvements in generation capabilities, particularly during fine-tuning, even without adjusting other hyperparameters. Additionally, the work introduces a general 'score smoothing' technique and an enhanced 'masked logarithmic score' to effectively support label smoothing for arbitrary scoring rules.",
    "Methodology": "The core methodology involves distributing non-local scoring rules at the token level, leveraging the autoregressive decomposition of sequence probability. This transforms the sequence prediction task into a series of conditional token prediction tasks, reducing the sample space to the vocabulary size at each timestep and making non-local scores tractable. The paper uses the Logarithmic score as a baseline and introduces the Brier score and Spherical score as alternative loss functions. It also explores generalized forms like the alpha-power score and pseudo-spherical score. To enable regularization, a 'score smoothing' technique is proposed, defined as Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · Σ_j S(p, j). To counter the issue of mild penalties for bounded scores, a 'masked logarithmic score' Sϵ_log(p, i) = Sϵ(p, i) + ϵ/m Σ_j 1{pj < ϵ/m}log pj is introduced to enhance the smoothing effect by applying the logarithmic score to under-smooth labels.",
    "Experimental Setup": "The method was evaluated on various benchmarks: machine translation (WMT14 English-French, WMT14 English-German, TED bilingual dataset) and abstractive summarization (CNN/DailyMail dataset). Models used include Transformer-base and Transformer-big for smaller scale experiments, and LLaMA-7B and LLaMA-13B for large language model instruction tuning. Performance was measured using BLEU for machine translation, ROUGE-1, ROUGE-2, and ROUGE-L for summarization, and MT-bench for multi-turn question answering capabilities of LLMs. Experiments involved training models from scratch with different scoring rules, fine-tuning pre-trained models (with logarithmic score) using Brier and Spherical scores, and instruction tuning LLMs on the Alpaca dataset (52K demonstrations) by replacing the log-likelihood loss. Standard Transformer hyperparameters were used without specific tuning for alternative scores, and beam search was employed for decoding.",
    "Limitations": "The logarithmic score still performs best when training models from scratch, suggesting that hyperparameters optimized for it may not be ideal for other scoring rules. Alternative scores, being bounded, may impose a 'relatively mild penalty' when score smoothing is ignored, potentially reducing the model's incentive to perform label smoothing. Due to high computational costs, LLMs were only instruction-tuned, not pre-trained from scratch. When used as beam search objectives, the logarithmic score generally yielded better performance than the Brier and Spherical scores, indicating that these alternative scores might not be universally superior for all aspects of language generation or decoding strategies.",
    "Future Research Directions": "Future work could investigate other strictly proper scores for potentially superior performance during pre-training or fine-tuning. It's crucial to identify the underlying factors contributing to performance differences among strictly proper scores and develop new metrics or properties to determine a score's suitability for language generation models. Exploring the utility of these alternative scores as evaluation metrics (similar to Perplexity) for assessing the calibration of language generation models is another promising direction. Additionally, further research into applying different scoring rules as objectives for beam search could reveal scenarios where they outperform the logarithmic score in decoding.",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': '• Closest prior art (“Language Generation with Strictly Proper Scoring Rules”) studies replacing cross-entropy with bounded proper scores (Brier, Spherical) and proposes score-smoothing and a masked-log variant, but it does not form a simple convex mixture of cross-entropy and Brier, nor does it schedule the mixing coefficient during training.  \n• The masked-logarithmic score still applies the log term only to low-probability classes and is tied to a particular smoothing hyper-parameter, whereas HyPro keeps the full cross-entropy signal and interpolates it continuously with a bounded rule, preserving strict propriety by construction.  \n• No prior LLM work reports a time-dependent blending of two proper token-level losses to obtain both fast convergence (early CE dominance) and final calibration (late Brier dominance).  \n• The preference-optimization paper (DiscoPOP) discovers a different pairwise reward loss for RLHF; it is orthogonal to token-level language-model training.  \n• Theoretical observation that a convex combination of proper scores remains proper, and the empirical finding of CE-like PPL together with Brier-like calibration, are not documented in earlier literature.  \nTherefore the method introduces a distinct yet simple idea not present in the cited works.', 'novelty_score': 7, 'significance_reason': '• Calibration of LLM probabilities affects confidence estimation, retrieval-augmented generation, chain-of-thought sampling, and safety filters; improving it without hurting likelihood is valuable for both academic research and real-world deployment.  \n• HyPro requires only one extra scalar schedule and one additional term in the loss; it is architecture-agnostic, compatible with existing optimisers, and thus immediately usable across pre-training, fine-tuning, and instruction-tuning regimes, lowering adoption barriers.  \n• Because training speed and final perplexity match the cross-entropy baseline, the method avoids the main obstacle that prevented wider use of bounded scores.  \n• The idea can stimulate further theoretical work on mixtures of proper scores and practical work on adaptive or task-specific blending, giving it academic impact.  \n• Societally, better-calibrated LLMs can provide more reliable confidence estimates to end-users, potentially reducing misinformation and enabling safer autonomous decision pipelines.  \n• Limitation: demonstrated only on GPT-2 / WikiText-2; additional large-scale evidence is required, so the immediate impact is promising but not yet transformative.', 'significance_score': 8}
