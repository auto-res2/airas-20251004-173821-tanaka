
Input:
You are a researcher in machine learning. Based on the instructions below, please generate a simple new research method with minimal modifications to existing approaches.

# Instructions:
- Read the research topic described below:
    LLMの新しい損失関数
- A list of related prior studies is provided. Each entry contains a summary of its title, main contributions, methodologies, results, and limitations:
    {
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "This paper introduces an LLM-driven objective discovery pipeline to automatically generate novel state-of-the-art offline preference optimization algorithms for Large Language Models (LLMs) without expert human intervention. The main research problem addressed is the under-exploration of the vast search space of possible loss functions in preference optimization, traditionally constrained by human creativity. The key finding is the discovery of Discovered Preference Optimization (DiscoPOP), a novel algorithm (also referred to as Log Ratio Modulated Loss, LRML) that adaptively blends logistic and exponential losses. Experiments demonstrate DiscoPOP's state-of-the-art performance and its successful transfer to held-out tasks.",
    "Methodology": "The core methodology is an iterative LLM-driven objective discovery pipeline. An LLM (GPT-4) is iteratively prompted to propose new preference optimization loss functions as PyTorch code. The process starts by providing the LLM with established loss functions and their performance as in-context examples. Each proposed function undergoes unit tests for validity. Valid functions are used to fine-tune an LLM, and their performance is evaluated on a predefined downstream validation task (MT-Bench). This performance metric is then fed back to the LLM to refine subsequent proposals. This meta-optimization approach aims to discover novel learning algorithms by leveraging the LLM's knowledge to generate code-level objective functions directly. The discovered DiscoPOP (LRML) dynamically weights logistic and exponential losses based on a sigmoid calculation of the difference of log-ratios, with a temperature parameter.",
    "Experimental Setup": "The LLM-driven discovery process was performed using GPT-4. The base LLM for preference optimization was `zephyr-7b-gemma-sft` (a 7 billion Gemma model fine-tuned on `deita-10k-v0-sft`), and for the IMDb task, a GPT-2 model fine-tuned on the IMDb dataset. Training utilized a fixed `β = 0.05` during discovery, a learning rate of 5e-7, bfloat16, two epochs, a batch size of two per device, gradient accumulation of eight, a cosine learning rate scheduler, and the AdamW optimizer, implemented with the TRL transformers library. Models were trained on Nvidia A100 GPUs (8 for discovery, 4 for summarization and IMDb). The discovery task focused on multi-turn dialogue, evaluated on MT-Bench using GPT-4 as a judge. Held-out tasks included single-turn dialogue (Alpaca Eval 2.0, evaluated against GPT-4 or SFT base model with win rates), summarization (TL;DR dataset, custom evaluation with Alpaca Eval 2.0 library and GPT-4 annotator), and positive sentiment generation (IMDb, evaluated using a pre-trained sentiment classifier and KL-Divergence across various `β` values).",
    "Limitations": "The current approach has several limitations: the exploration of LLM objective proposal generation techniques is still nascent; DiscoPOP struggles to converge when the `β` parameter is too low (≤ 0.01) or too high (≥ 2.5), likely because the discovery process was fixed at `β=0.05`; the highest-performing loss re-purposed `β`, affecting both functional behavior and KL penalty; the use of closed-source models (GPT-4) for code generation limits reproducibility and is costly; LLM outputs are prone to hallucinations, necessitating content filters; and the work takes a step towards code-level self-improvement, which could lead to unintended behaviors.",
    "Future Research Directions": "Future work could focus on more effective LLM objective proposal generation techniques, such as leveraging entire learning curve plots with Visual Language Models or meta-meta-optimizing the LLM prompt. Exploring objective functions with multiple floating-point parameters that can be tuned separately, beyond a single `β` parameter, is also suggested. Reformulating the objective to enable multi-parameter analysis would be beneficial. Furthermore, using the produced models themselves to generate code for self-improvement is a potential avenue. Lastly, constraining the exploring LLM to ensure the `β` multiplication with the difference of log-ratios `ρ` before other calculations is crucial.",
    "Experiment Code": "    def log_ratio_modulated_loss(\n        self,\n        policy_chosen_logps: torch.FloatTensor,\n        policy_rejected_logps: torch.FloatTensor,\n        reference_chosen_logps: torch.FloatTensor,\n        reference_rejected_logps: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        tau = 0.05\n        pi_logratios = policy_chosen_logps - policy_rejected_logps\n        ref_logratios = reference_chosen_logps - reference_rejected_logps\n        logits = pi_logratios - ref_logratios\n        logits = logits * self.beta\n        # Modulate the mixing coefficient based on the log ratio magnitudes\n        log_ratio_modulation = torch.sigmoid(logits / tau)\n        logistic_component = -F.logsigmoid(logits)\n        exp_component = torch.exp(-logits)\n        # Blend between logistic and exponential component based on log ratio modulation\n        losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n        return losses",
    "Experiment Result": "The iterative LLM-driven objective discovery pipeline uses GPT-4 as the LLM for proposing new preference optimization loss functions. The process is initiated with an archive of established loss functions (DPO, HINGE, IPO, KTO) and their MT-Bench performance scores as in-context examples.\n\nEach proposed loss function undergoes a unit test for validity, checking for proper function definition, callable status, output shape of (10,), and absence of NaNs in both the loss output and its gradients after a backward pass.\n\nValid loss functions are then used to fine-tune a model (e.g., Zephyr 2B or 7B Gemma) using the `scripts/run_gpo.py` script, leveraging `accelerate launch` with `deepspeed_zero3.yaml` for distributed training. Training parameters include `gradient_accumulation_steps=16` (when 4 GPUs are used) and a `beta` parameter fixed at `0.05` for the loss functions.\n\nThe fine-tuned models' performance is evaluated on the MT-Bench downstream validation task using FastChat's `llm_judge` tools (`gen_model_answer.py` and `gen_judgment.py`). The average score from MT-Bench serves as the fitness metric, which is then fed back to GPT-4 to inform and refine subsequent loss function proposals.\n\nThe discovered DiscoPOP (LRML) loss function itself incorporates a temperature parameter `tau = 0.05` to modulate the mixing coefficient between its logistic and exponential components based on a sigmoid calculation of the difference of log-ratios."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "The paper proposes a novel strategy to adapt any non-local strictly proper scoring rules for language generation, addressing the limitation of the widely used logarithmic score (cross-entropy loss) in handling exponentially large sample spaces. It demonstrates that training language generation models, including large language models (LLMs) like LLaMA-7B and LLaMA-13B, with alternative strictly proper scoring rules (Brier score and Spherical score) can yield substantial improvements in generation capabilities, particularly during fine-tuning, even without adjusting other hyperparameters. Additionally, the work introduces a general 'score smoothing' technique and an enhanced 'masked logarithmic score' to effectively support label smoothing for arbitrary scoring rules.",
    "Methodology": "The core methodology involves distributing non-local scoring rules at the token level, leveraging the autoregressive decomposition of sequence probability. This transforms the sequence prediction task into a series of conditional token prediction tasks, reducing the sample space to the vocabulary size at each timestep and making non-local scores tractable. The paper uses the Logarithmic score as a baseline and introduces the Brier score and Spherical score as alternative loss functions. It also explores generalized forms like the alpha-power score and pseudo-spherical score. To enable regularization, a 'score smoothing' technique is proposed, defined as Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · Σ_j S(p, j). To counter the issue of mild penalties for bounded scores, a 'masked logarithmic score' Sϵ_log(p, i) = Sϵ(p, i) + ϵ/m Σ_j 1{pj < ϵ/m}log pj is introduced to enhance the smoothing effect by applying the logarithmic score to under-smooth labels.",
    "Experimental Setup": "The method was evaluated on various benchmarks: machine translation (WMT14 English-French, WMT14 English-German, TED bilingual dataset) and abstractive summarization (CNN/DailyMail dataset). Models used include Transformer-base and Transformer-big for smaller scale experiments, and LLaMA-7B and LLaMA-13B for large language model instruction tuning. Performance was measured using BLEU for machine translation, ROUGE-1, ROUGE-2, and ROUGE-L for summarization, and MT-bench for multi-turn question answering capabilities of LLMs. Experiments involved training models from scratch with different scoring rules, fine-tuning pre-trained models (with logarithmic score) using Brier and Spherical scores, and instruction tuning LLMs on the Alpaca dataset (52K demonstrations) by replacing the log-likelihood loss. Standard Transformer hyperparameters were used without specific tuning for alternative scores, and beam search was employed for decoding.",
    "Limitations": "The logarithmic score still performs best when training models from scratch, suggesting that hyperparameters optimized for it may not be ideal for other scoring rules. Alternative scores, being bounded, may impose a 'relatively mild penalty' when score smoothing is ignored, potentially reducing the model's incentive to perform label smoothing. Due to high computational costs, LLMs were only instruction-tuned, not pre-trained from scratch. When used as beam search objectives, the logarithmic score generally yielded better performance than the Brier and Spherical scores, indicating that these alternative scores might not be universally superior for all aspects of language generation or decoding strategies.",
    "Future Research Directions": "Future work could investigate other strictly proper scores for potentially superior performance during pre-training or fine-tuning. It's crucial to identify the underlying factors contributing to performance differences among strictly proper scores and develop new metrics or properties to determine a score's suitability for language generation models. Exploring the utility of these alternative scores as evaluation metrics (similar to Perplexity) for assessing the calibration of language generation models is another promising direction. Additionally, further research into applying different scoring rules as objectives for beam search could reveal scenarios where they outperform the logarithmic score in decoding.",
    "Experiment Code": null,
    "Experiment Result": null
}
- Identify the most promising existing method that can be improved with minimal modifications to its objective function or core algorithm.
- Propose a new method that requires only small, focused changes to the existing approach (e.g., adding a regularization term, modifying the loss function, or introducing a simple weighting mechanism).
- Ensure the proposed method can be validated with a simple Python experiment.

# Output content:
Based on the above analysis, propose a simple new research method that advances the field through minimal but effective modifications. Your output should include:

- open_problems
    - Identify the key limitation in existing methods that can be addressed with minimal modifications.
    - Focus on problems that can be solved through simple changes to objective functions or algorithms.

- methods
    - Describe the minimal modification to the existing method (e.g., adding regularization, modifying loss function).
    - Explain the theoretical motivation for this change.
    - Keep the modification simple and focused on the identified problem.

- experimental_setup
    - Provide a concrete but simple experimental design.
    - Specify which datasets and evaluation metrics will be used.
    - Design a straightforward comparison with the base method.

- experimental_code
    - Output the core Python code implementing the proposed modification.
    - Focus only on the key changes to the base method.
    - Keep the code concise and readable.

- expected_result
    - Describe the expected experimental results and performance improvement over the base method.

- expected_conclusion
    - Summarize the practical value of the minimal modification.
    - Explain why this simple change leads to meaningful improvement.
Output:
{
    "open_problems": "Alternative strictly-proper scoring rules (e.g. Brier, Spherical) improve calibration and downstream generation when used in fine-tuning, but their bounded nature gives weak gradients, slowing convergence and sometimes hurting perplexity. Conversely, the usual cross-entropy (logarithmic score) learns fast but produces over-confident, poorly calibrated probabilities. The key open problem is to obtain fast learning and good perplexity of cross-entropy while retaining the calibration benefits of bounded scores – ideally through a very small change to the loss.",
    "methods": "We propose Hybrid Proper Loss (HyPro): a convex combination of the token-level Cross-Entropy (CE) and the Brier score.  \nToken at time t with true index y and predicted distribution p:\n  CE_t = − log p_y\n  Brier_t = (1 − p_y)^2 + Σ_{j≠y} p_j^2\n  HyPro_t = (1 − α) · CE_t + α · Brier_t ,  0 ≤ α ≤ 1\n\nThe mixture of strictly proper scores is itself strictly proper, so HyPro remains a valid training objective.  \nTheoretical motivation:\n1. Early training requires strong gradients ⇒ keep CE dominant.\n2. Later training benefits from calibration ⇒ slowly increase α.\nWe therefore schedule α linearly from 0 to α_max (e.g. 0.3) over the first K% of steps.\nNo other change to optimiser, model, or data is required.",
    "experimental_setup": "Model: GPT-2 small (117 M).  \nDataset: WikiText-2 for language modeling fine-tuning (train/valid/test splits).  \nBaselines: (a) Cross-Entropy, (b) pure Brier (α=1).  \nProposed: HyPro with α scheduled 0→0.3 over first 50% steps.\nMetrics:  \n1. Perplexity (PPL) on validation/test.  \n2. Expected Calibration Error (ECE) with 10 bins.  \nTraining: 3 epochs, LR 5e-5, AdamW, batch 8, context length 512.  \nCompare PPL and ECE of HyPro against baselines.",
    "experimental_code": "import torch, torch.nn.functional as F\n\ndef brier_loss(logits, targets):\n    # logits: [B, V], targets: [B]\n    probs = torch.softmax(logits, dim=-1)\n    one_hot = F.one_hot(targets, logits.size(-1)).float()\n    return ((probs - one_hot) ** 2).sum(dim=-1)  # [B]\n\ndef hybrid_loss(logits, targets, alpha):\n    ce = F.cross_entropy(logits, targets, reduction='none')\n    br = brier_loss(logits, targets)\n    return (1 - alpha) * ce + alpha * br  # [B]\n\n# training step skeleton\na_max = 0.3\nwarmup_fraction = 0.5\n\ndef get_alpha(step, total_steps):\n    progress = step / (total_steps * warmup_fraction)\n    return min(a_max, a_max * progress)\n\n# in the training loop\nfor step, (input_ids, targets) in enumerate(loader):\n    logits = model(input_ids).logits[:, -1]  # last token prediction\n    alpha = get_alpha(step, total_steps)\n    loss = hybrid_loss(logits, targets, alpha).mean()\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()",
    "expected_result": "1. Perplexity: HyPro ≈ CE (within ±0.5 PPL) and markedly better than pure Brier.\n2. Calibration: HyPro reduces ECE by 20-30 % relative to CE and is close to (or slightly better than) pure Brier.\nThus, HyPro achieves CE-like likelihood performance and Brier-like calibration simultaneously.",
    "expected_conclusion": "A simple weighted mix of two existing strictly proper losses yields the best of both worlds: fast, stable learning and well-calibrated output probabilities. Because the modification is a single extra line in the loss computation, it can be dropped into any current LLM training or fine-tuning pipeline, providing immediate practical benefits without hyper-parameter retuning or architectural changes."
}
