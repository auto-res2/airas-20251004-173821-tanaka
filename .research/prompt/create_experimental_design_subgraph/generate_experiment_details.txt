
Input:
You are an AI researcher. You will conduct experiments to demonstrate the superiority of the new method described in # New Methods. Please output all information required to implement the experiments according to the format specified in # Output Format. The section # Experimental Environment describes the computational environment available for this experiment.

# Experimental Environment
NVIDIA A100
VRAM：80GB
RAM：2048 GB

# Current Research Method (Target for Experiment Design)
{
    "Open Problems": "Alternative strictly-proper scoring rules (e.g. Brier, Spherical) improve calibration and downstream generation when used in fine-tuning, but their bounded nature gives weak gradients, slowing convergence and sometimes hurting perplexity. Conversely, the usual cross-entropy (logarithmic score) learns fast but produces over-confident, poorly calibrated probabilities. The key open problem is to obtain fast learning and good perplexity of cross-entropy while retaining the calibration benefits of bounded scores – ideally through a very small change to the loss.",
    "Methods": "We propose Hybrid Proper Loss (HyPro): a convex combination of the token-level Cross-Entropy (CE) and the Brier score.  \nToken at time t with true index y and predicted distribution p:\n  CE_t = − log p_y\n  Brier_t = (1 − p_y)^2 + Σ_{j≠y} p_j^2\n  HyPro_t = (1 − α) · CE_t + α · Brier_t ,  0 ≤ α ≤ 1\n\nThe mixture of strictly proper scores is itself strictly proper, so HyPro remains a valid training objective.  \nTheoretical motivation:\n1. Early training requires strong gradients ⇒ keep CE dominant.\n2. Later training benefits from calibration ⇒ slowly increase α.\nWe therefore schedule α linearly from 0 to α_max (e.g. 0.3) over the first K% of steps.\nNo other change to optimiser, model, or data is required.",
    "Experimental Setup": "Model: GPT-2 small (117 M).  \nDataset: WikiText-2 for language modeling fine-tuning (train/valid/test splits).  \nBaselines: (a) Cross-Entropy, (b) pure Brier (α=1).  \nProposed: HyPro with α scheduled 0→0.3 over first 50% steps.\nMetrics:  \n1. Perplexity (PPL) on validation/test.  \n2. Expected Calibration Error (ECE) with 10 bins.  \nTraining: 3 epochs, LR 5e-5, AdamW, batch 8, context length 512.  \nCompare PPL and ECE of HyPro against baselines.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef brier_loss(logits, targets):\n    # logits: [B, V], targets: [B]\n    probs = torch.softmax(logits, dim=-1)\n    one_hot = F.one_hot(targets, logits.size(-1)).float()\n    return ((probs - one_hot) ** 2).sum(dim=-1)  # [B]\n\ndef hybrid_loss(logits, targets, alpha):\n    ce = F.cross_entropy(logits, targets, reduction='none')\n    br = brier_loss(logits, targets)\n    return (1 - alpha) * ce + alpha * br  # [B]\n\n# training step skeleton\na_max = 0.3\nwarmup_fraction = 0.5\n\ndef get_alpha(step, total_steps):\n    progress = step / (total_steps * warmup_fraction)\n    return min(a_max, a_max * progress)\n\n# in the training loop\nfor step, (input_ids, targets) in enumerate(loader):\n    logits = model(input_ids).logits[:, -1]  # last token prediction\n    alpha = get_alpha(step, total_steps)\n    loss = hybrid_loss(logits, targets, alpha).mean()\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()",
    "Expected Result": "1. Perplexity: HyPro ≈ CE (within ±0.5 PPL) and markedly better than pure Brier.\n2. Calibration: HyPro reduces ECE by 20-30 % relative to CE and is close to (or slightly better than) pure Brier.\nThus, HyPro achieves CE-like likelihood performance and Brier-like calibration simultaneously.",
    "Expected Conclusion": "A simple weighted mix of two existing strictly proper losses yields the best of both worlds: fast, stable learning and well-calibrated output probabilities. Because the modification is a single extra line in the loss computation, it can be dropped into any current LLM training or fine-tuning pipeline, providing immediate practical benefits without hyper-parameter retuning or architectural changes."
}

# MODEL LIST
{
    "Large Language Models": {
        "Llama-4-Scout-17B-16E": "Llama 4, developed by Meta, is a new generation of natively multimodal AI models that leverage a Mixture-of-Experts (MoE) architecture to achieve state-of-the-art performance in both text and image understanding. Marking the beginning of a new era for the Llama ecosystem, the series introduces two models: Llama 4 Scout, a 17-billion-parameter model with 16 experts, and Llama 4 Maverick, also with 17 billion parameters but incorporating 128 experts. These auto-regressive language models employ early fusion to enable seamless multimodal processing, allowing them to integrate text and image information natively.",
        "Llama-4-Maverick-17B-128E": "The Llama 4 collection, developed by Meta, represents a new generation of natively multimodal AI models designed to enable both text and multimodal experiences. By leveraging a Mixture-of-Experts (MoE) architecture, these models deliver industry-leading performance in understanding text and images. Marking the beginning of a new era for the Llama ecosystem, the series introduces two efficient models: Llama 4 Scout, a 17-billion-parameter model with 16 experts, and Llama 4 Maverick, also with 17 billion parameters but featuring 128 experts. Built as auto-regressive language models, the Llama 4 series incorporates early fusion to achieve seamless and native multimodality.",
        "Qwen3-0.6B": "Qwen3-0.6B is a compact causal language model with 0.6B parameters, offering dense and MoE variants, improved reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and 32K context length.",
        "Qwen3-1.7B": "Qwen3-1.7B is a next-generation causal language model with 1.7B parameters, offering dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and long-context processing up to 32K tokens.",
        "Qwen3-4B": "Qwen3 is the latest generation of large language models, featuring both dense and MoE variants with enhanced reasoning, instruction-following, agent capabilities, and multilingual support, including seamless mode switching, superior alignment, and long-context processing up to 131K tokens.",
        "Qwen3-8B": "Qwen3-8B is an advanced causal language model with 8.2B parameters, featuring dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and long-context processing up to 131K tokens.",
        "Qwen3-14B": "Qwen3-14B is a large-scale causal language model with 14.8B parameters, offering dense and MoE variants, advanced reasoning, seamless mode switching, strong alignment, agent capabilities, multilingual support, and extended context handling up to 131K tokens.",
        "Qwen3-32B": "Qwen3-32B is a powerful causal language model with 32.8B parameters, featuring dense and MoE variants, enhanced reasoning, seamless mode switching, strong alignment, advanced agent capabilities, multilingual support, and long-context processing up to 131K tokens.",
        "DeepSeek-v3": "DeepSeek-V3 is a 671B-parameter MoE language model (37B active per token) featuring MLA and DeepSeekMoE architectures, auxiliary-loss-free load balancing, and multi-token prediction, trained on 14.8T tokens with efficient GPU usage, achieving performance comparable to top closed-source models while maintaining stable training.",
        "DeepSeek-V3.1": "DeepSeek-V3.1 extends DeepSeek-V3 with larger long-context training (630B tokens at 32K and 209B tokens at 128K) and adopts FP8 data formats for efficiency and compatibility.",
        "DeepSeek-V3.2-Exp": "DeepSeek-V3.2-Exp, built on V3.1-Terminus, introduces Sparse Attention to improve training and inference efficiency for long-context processing as part of ongoing research into more efficient transformer architectures.",
        "gpt-oss-20b": "The gpt-oss series, introduced by OpenAI, consists of open-weight models designed to support powerful reasoning, agentic tasks, and a wide range of developer use cases. Two versions are being released: gpt-oss-120b, a 117-billion-parameter model with 5.1 billion active parameters optimized for production-level, general-purpose, high-reasoning tasks that can fit into a single 80GB GPU such as the NVIDIA H100 or AMD MI300X; and gpt-oss-20b, a 21-billion-parameter model with 3.6 billion active parameters intended for lower latency, as well as local or specialized applications. Both models were trained using OpenAI’s harmony response format and must be used with this format to function correctly.",
        "gemma-3-1b-it": "Gemma is a family of lightweight open models from Google, built on the same research as Gemini. The latest Gemma 3 models are multimodal, supporting both text and image inputs with text generation outputs. They feature a 128K context window (32K for the 1B model), multilingual support in over 140 languages, and come in multiple sizes, making them suitable for tasks such as question answering, summarization, reasoning, and image understanding. Their smaller size allows deployment on laptops, desktops, or personal cloud infrastructure, broadening access to advanced AI. Inputs include text and images normalized to 896×896 resolution, while outputs are generated text with up to 8192 tokens of context.",
        "gemma-3-4b-it": "Gemma is a family of lightweight open models from Google, built on the same research behind the Gemini models. The latest Gemma 3 models are multimodal, capable of processing both text and images as input and generating text as output, with open weights for pre-trained and instruction-tuned variants. They offer a 128K context window (32K for the 1B model), support over 140 languages, and come in more sizes than earlier versions, making them suitable for tasks like question answering, summarization, reasoning, and image understanding. Thanks to their smaller size, Gemma models can run on laptops, desktops, or personal cloud setups, expanding access to advanced AI. Inputs include text and images (normalized to 896×896 and encoded to 256 tokens each), while outputs are generated text with up to 8192 tokens.",
        "gemma-3-27b-it": "Gemma is a family of lightweight open models from Google, built on the same research as the Gemini models. The Gemma 3 series is multimodal, able to take both text and image inputs and generate text outputs, with open weights available for both pre-trained and instruction-tuned versions. They feature a 128K context window (32K for the 1B model), multilingual support in more than 140 languages, and come in a wider range of sizes than previous releases. Well-suited for tasks such as question answering, summarization, reasoning, and image understanding, Gemma models are compact enough to run on laptops, desktops, or personal cloud setups, making advanced AI more broadly accessible. Inputs include text strings or images normalized to 896×896 and encoded into 256 tokens each, while outputs are generated text of up to 8192 tokens."
    },
    "Vision Language Models": {},
    "Vision Language Action Models": {},
    "Diffusion Models": {}
}

# DATASET LIST
{
    "Text Datasets": {
        "alpaca-cleaned": "",
        "databricks-dolly-15k": ""
    },
    "Image Datasets": {
        "ImageNet": "",
        "CIFAR-10": ""
    }
}

# Output Format
- experiment_summary：
  - Describe the overall implementation details of the experiment. Summarize the purpose, components, and workflow so that the entire structure of the experiment can be clearly understood.
- evaluation_metrics：
  - List all evaluation metrics used in this experiment, including only their names, in a list format. (e.g., Accuracy AUC ROC, F1 Score, RMSE, BLEU, ROUGE, etc.)
- models_to_use：
  - Select 1 deep learning or machine learning models to be used in the experiment and output them in a list format.
  - Each model name should clearly indicate its number of parameters.
  - Refer to the provided “# MODEL LIST” for guidance, although models not included in the list are also acceptable.
  - If the proposed method itself introduces a new model (e.g., a novel architecture), return an empty list and describe the details of the method in new_method.
- datasets_to_use：
  - Select 1 datasets to be used in the experiment and output them in a list format.
  - Refer to the provided “# DATASET LIST” for guidance, although datasets not included in the list are also acceptable.
  - If a new dataset is proposed as part of this study, return an empty list and describe its details in new_method.
- new_method：
  - Describe the proposed method and its implementation in detail.
  - Clearly state its objectives, theoretical background, components, and algorithmic procedures.
- comparative_methods：
  - Select 1 existing methods for comparison with the proposed method and output them in a list format.
  - For example, if the proposed method is a new optimization algorithm, comparative methods might include Adam or AdamW.
  - If the proposal is a new LLM architecture, comparative methods might include Llama 4 or Qwen.
- hyperparameters_to_search：
  - List all hyperparameters to be explored, including only their names, in a list format.
Output:
{
    "experiment_summary": "This experiment investigates whether Hybrid Proper Loss (HyPro) – a scheduled convex combination of the token-level Cross-Entropy (CE) and Brier losses – can preserve the fast convergence and low perplexity of CE while inheriting the strong probability calibration of bounded strictly-proper scores. \n\nWorkflow\n1. Model: fine-tune a pretrained autoregressive language model (GPT-2 Small, 117 M parameters) on an English Wikipedia subset (WikiText-2).\n2. Loss functions:\n   • HyPro (proposed) – start with CE dominance (α=0) and linearly increase the Brier weight to α_max over the first K % of total optimisation steps.\n   • Cross-Entropy (baseline, used for comparative evaluation only).\n3. Training setup: 3 epochs, AdamW, learning-rate 5 × 10⁻⁵, batch 8, context 512, sequence-parallel data loading; fits well inside a single NVIDIA A100 (80 GB) with ample CPU RAM for pre-fetching.\n4. Hyper-parameter search: sweep α_max and warm-up fraction to locate the best calibration-vs-likelihood trade-off while monitoring validation perplexity for early stopping.\n5. Evaluation on the held-out test split: measure Perplexity (PPL) and Expected Calibration Error (ECE, 10 equal-width confidence bins).\n6. Compare HyPro with standard CE to demonstrate that HyPro attains comparable PPL (±0.5) and ≈25 % lower ECE.\n\nThe entire pipeline can be reproduced with a lightweight PyTorch script, a single loss-wrapper function, and an alpha-scheduler.",
    "evaluation_metrics": [
        "Perplexity",
        "Expected Calibration Error"
    ],
    "models_to_use": [
        "GPT-2 Small (117 M)"
    ],
    "datasets_to_use": [
        "WikiText-2"
    ],
    "new_method": "Hybrid Proper Loss (HyPro)\nObjective: Combine the strong optimisation signal of Cross-Entropy with the calibration benefits of the Brier score using a time-dependent mixture coefficient.\n\nDefinition for a token t with ground-truth index y and prediction probabilities p (logits l):\n  CE_t  =  −log p_y\n  Brier_t = (1 − p_y)² + Σ_{j≠y} p_j²\n  HyPro_t = (1 − α)·CE_t + α·Brier_t ,   0 ≤ α ≤ 1\n\nScheduling rule:\n  α(step) = min(α_max , α_max · step / (K · total_steps))\nwhere K is the warm-up percentage (e.g., K = 0.5 → first half of training).  After warm-up, α stays fixed at α_max.\n\nAlgorithmic procedure (per optimisation step):\n1. Forward pass: obtain logits for the next-token distribution.\n2. Compute α using the scheduler.\n3. Compute HyPro loss per token as above; average over the batch.\n4. Back-propagate; update parameters with AdamW.\n\nThe resulting loss remains strictly proper, guaranteeing that the true distribution minimises expected loss, and requires only one extra line of code relative to CE.",
    "comparative_methods": [
        "Cross-Entropy Loss"
    ],
    "hyperparameters_to_search": [
        "alpha_max",
        "warmup_fraction",
        "learning_rate",
        "batch_size"
    ]
}
